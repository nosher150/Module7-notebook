{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nosher150/Module7-notebook/blob/main/Introducing_Dataframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k4aZ-G2hX-g"
      },
      "source": [
        "# Introduction to Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHVDHfV1hX-j"
      },
      "source": [
        "Python is a jack of all trades language that allows you to build functions that can do almost anything you want. But how what benefits does it have for our work as data analysts?<br> <br>It turns out, quite alot!<br><br> You can perform any of the data cleaning, analysis or visualisation techniques you have learned already in a way that is much more efficient and fine tuned to what you need. This is where pandas come in. <br><br> Pandas is an open source library that is used to mainly for data analysis. It allows us to import and view data, as well as perform a wide range of explorative or analytical techniques. Ontop of this, pandas will be used as the base building block of everything we do in subsequent modules. <br><br> In this workbook we will show you how to import data with pandas and how to perform some basic data cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05hfpaF0hX-l"
      },
      "outputs": [],
      "source": [
        "# Importing pandas is the same as any other library. \n",
        "# Notice that the alias pd is used, this is an industry standard and something you should get in the habit of doing\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTGDToCXhX-n"
      },
      "source": [
        "Before we go further we need to define a couple of terms within pandas<br><br>\n",
        "\n",
        "## Series\n",
        "\n",
        "Technically, a series is a one dimensional array holding data of any type. Simply, if you imagine a pandas object as a table, then a series is a column. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UPiHQFGhX-o"
      },
      "outputs": [],
      "source": [
        "a=['Jess','Garfield','Snowball II']\n",
        "\n",
        "my_series=pd.Series(a)\n",
        "\n",
        "print(my_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl9SdTwqhX-o"
      },
      "source": [
        "Like a list, a series is 0-indexed so if we wanted to retrieve any element of a series we just need to call the relevant index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKYFoU7HhX-p"
      },
      "outputs": [],
      "source": [
        "print(my_series[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3zY4mx7hX-p"
      },
      "source": [
        "Unlike a list, we can actually name the indices within a series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvsjZUgdhX-q"
      },
      "outputs": [],
      "source": [
        "my_series=pd.Series(a,index=['Postman Pat','Jon Arbuckle','Homer Simpson'])\n",
        "print(my_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrfP4LOshX-r"
      },
      "outputs": [],
      "source": [
        "print(my_series['Postman Pat'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cV0k88hhX-r"
      },
      "source": [
        "## Dataframe\n",
        "\n",
        "If a series is a column, then a dataframe is the table itself. These are two-dimensional arrays, which consist of at least one series. For many people it is easier to think of dataframes as tables with columns and rows. <br><br>\n",
        "\n",
        "There are several ways we can build a dataframe manually, one of the simplest is to use a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCEjsAmxhX-s"
      },
      "outputs": [],
      "source": [
        "data={'Owner':['Postman Pat','Jon Arbuckle','Homer Simpson'],'Pet':['Jess','Garfield','Snowball II']}\n",
        "\n",
        "df=pd.DataFrame(data)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK-GzdIjhX-s"
      },
      "source": [
        "Notice that the keys from the dictionary are now the column names, we will come back to this later to show you how you can change column names. <br><br>\n",
        "\n",
        "Most of the time though, we aren't going to be building dataframes manually. Normally we will have want to load the dataset in. Pandas has inbuilt functions for several types of file, which you can read about in their <a href='https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html'>documentation</a>. The one we will be using in this course is how to read in a CSV file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nArOpJ6ehX-t"
      },
      "outputs": [],
      "source": [
        "# The dataset we will be using in this workbook contains records of over 80000 UFO sightings\n",
        "df=pd.read_csv('Data/ufo_sighting_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgnBi8erhX-t"
      },
      "source": [
        "Note that within a series, all data types must be the same. If this isn't consistent (as in this case), pandas will automatically force them to be the same. In this case it will set the conflicting column to an object (string)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDqHszskhX-t"
      },
      "source": [
        "If you would like to read about importing Excel files, you can read the documentation <a href='https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html'>here</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJGHNk1xhX-u"
      },
      "source": [
        "# Inspecting the Data\n",
        "\n",
        "Now we know how to load our data into pandas, it is time to look at the various functions pandas has for inspecting the data. <br><br>\n",
        "\n",
        "As before, whenever you receive data you should inspect it to understand what it contains, what you can do with it and how it might need to be cleaned. In this section we will show you some basic pandas functions for inspecting our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL8Mf6XWhX-u"
      },
      "outputs": [],
      "source": [
        "# .head() allows us to view the first 5 rows of a dataframe, this can be extended to a maximum of 20\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opFMDiYhhX-v"
      },
      "outputs": [],
      "source": [
        "# .tail() does the same thing, but for the last 5 rows\n",
        "# Both functions are useful for getting a quick look at the data to see what is contained\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I0RDsHdhX-v"
      },
      "outputs": [],
      "source": [
        "# To understand how big our dataframe is, we can use .shape to return the number of rows and columns\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONvYOJ77hX-w"
      },
      "outputs": [],
      "source": [
        "# For a quick view of what data types are present, use .dtypes\n",
        "\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFA2K1VPhX-w"
      },
      "outputs": [],
      "source": [
        "# For a quick summary of what is happening in each series (column) you can use .describe()\n",
        "# By default this will only display series where the datatype is numeric (float or integer)\n",
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWBShrYOhX-w"
      },
      "outputs": [],
      "source": [
        "# To look at series with non-numeric datatypes you can set the parameter include to 'all'\n",
        "# Note that for non-numeric datatypes it uses new summaries (unique, top, freq) and does not include those for numerics\n",
        "\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPe1-WbXhX-x"
      },
      "outputs": [],
      "source": [
        "# For a more overall look at your different columns, use .info()\n",
        "# This shows for each column what the datatype is and how many non-nulls there are\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZZh8J0YhX-x"
      },
      "outputs": [],
      "source": [
        "# To view the column names, you use .columns\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i76jYehjhX-x"
      },
      "source": [
        "You may have noticed that some functions have parenthesis at the end, while others don't. Functions with parenthesis are performing some sort of calculation or procedure, while those that don't are just printing what is already there.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NhV4aichX-x"
      },
      "source": [
        "# Viewing Specific Data\n",
        "\n",
        "So far we have looked at ways of inspecting the whole dataframe, but what if we wanted to look at something more specific?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2DM9OR1hX-x"
      },
      "outputs": [],
      "source": [
        "# To look at a specific column, you can use the syntax df.column_name\n",
        "# Note this returns a series\n",
        "\n",
        "df.city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr9lEUI3hX-x"
      },
      "outputs": [],
      "source": [
        "# Alternatively, you can also use the syntax df[column_name]\n",
        "# This again will return a series, this is the only viable method for column names with whitespace, e.g. df['column name']\n",
        "\n",
        "df['city']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXR-KoRdhX-y"
      },
      "outputs": [],
      "source": [
        "# If you want to return a column in a new dataframe, then you need to double wrap the square brackets\n",
        "\n",
        "df[['city']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCt13xNnhX-y"
      },
      "outputs": [],
      "source": [
        "# This method therefore allows you to return more than one column as a dataframe is two-dimensional\n",
        "\n",
        "df[['city','country']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdlBdz8ghX-y"
      },
      "outputs": [],
      "source": [
        "# A very powerful function for returning specific parts of a dataframe is to use .loc\n",
        "# Think of it like using coordinates (row and column names) to specifically retrieve what you would like\n",
        "# It can be used to retreive a single value, a column, row or a subset of the dataframe\n",
        "\n",
        "df.loc[0:10,'city':'country']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31ByfM8EhX-y"
      },
      "outputs": [],
      "source": [
        "# Another method is to use .iloc, which uses indices instead of names to retrieve specific parts of a dataframe\n",
        "\n",
        "df.iloc[0:10,1:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J_M9Im_hX-y"
      },
      "outputs": [],
      "source": [
        "# To look at the frequency each item appears in a column we can use .value_counts()\n",
        "\n",
        "df.country.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niKD4tAShX-y"
      },
      "outputs": [],
      "source": [
        "# Finally, if you would like to see what the unique values are in a column you can use .unique()\n",
        "# Mostly useful for categorical data\n",
        "\n",
        "df.country.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2jhukj-hX-z"
      },
      "outputs": [],
      "source": [
        "# Practice: What city reported the most sightings?\n",
        "\n",
        "#A:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsDjl0t8hX-z"
      },
      "source": [
        "Click here to view solution<br><br>\n",
        "\n",
        "<p style=color:white> df.city.value_counts()</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fbKwnxXhX-z"
      },
      "source": [
        "# Filtering\n",
        "\n",
        "Now we know how to look at data within our dataframes, let's see how we can filter them to look at specific strata within the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0FPqhiIhX-z"
      },
      "outputs": [],
      "source": [
        "# The syntax for filtering data starts the exact same way as selecting a series, except now we add a condition\n",
        "# This returns for each value a True/False whether that condition has been met\n",
        "\n",
        "df.country=='us'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNmCKXnjhX-z"
      },
      "outputs": [],
      "source": [
        "# To then return a dataframe that returns the rows where this condition is True we use the following syntax\n",
        "\n",
        "df[df.country=='us'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yL6fJEEhX-0"
      },
      "source": [
        "You can use any type of condition that you have learned before, including >, <, >=, <=, !="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rrNyCCJhX-0"
      },
      "outputs": [],
      "source": [
        "# If you would like to add more than one condition you need to wrap each in parenthesis\n",
        "# If you want both conditions to apply you use & between the conditions\n",
        "\n",
        "df[(df.country=='us')&(df.UFO_shape=='circle')].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbZe2TrZhX-0"
      },
      "outputs": [],
      "source": [
        "# If you want either condition to be applied you place | between conditions\n",
        "\n",
        "df[(df.country=='us')|(df.country=='ca')].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHA4JhMLhX-0"
      },
      "source": [
        "# Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-903l59hX-0"
      },
      "outputs": [],
      "source": [
        "#1. Filter the dataframe to show UFO_shapes that are circle\n",
        "\n",
        "#A:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JNXA-i4hX-0"
      },
      "source": [
        "Click here to view solution<br><br>\n",
        "\n",
        "<p style=color:white> df[df.UFO_shape=='circle'] </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWvM_K43hX-0"
      },
      "outputs": [],
      "source": [
        "#2. Filter the dataframe to only show UFO sightings from Texas (tx)\n",
        "\n",
        "#A:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXS7g5jqhX-1"
      },
      "source": [
        "Click here to view solution<br><br>\n",
        "\n",
        "<p style=color:white> df[df['state/province']=='tx'] </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDUEWcHShX-1"
      },
      "outputs": [],
      "source": [
        "#3. Filter the data to show sightings that are either cylindrical or circular\n",
        "\n",
        "#A:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW_UNN3phX-1"
      },
      "source": [
        "Click here to view solution<br><br>\n",
        "\n",
        "<p style=color:white> df[(df.UFO_shape=='circle')|(df.UFO_shape=='cylinder')] </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA0i5OufhX-1"
      },
      "outputs": [],
      "source": [
        "#4. Filter the data to show sightings from the UK (gb) and last 300 seconds\n",
        "\n",
        "#A: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdFH2E-JhX-1"
      },
      "source": [
        "Click here to view solution<br><br>\n",
        "\n",
        "<p style=color:white> df[(df.country=='gb')&(df.length_of_encounter_seconds==300)] </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSMnHlTXhX-2"
      },
      "outputs": [],
      "source": [
        "#5. (Stretch) Filter the data to show sightings that are from outside the United States and are either spherical or light\n",
        "\n",
        "#A: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ls_L0P5hX-2"
      },
      "source": [
        "Click here to view solution<br><br>\n",
        "\n",
        "<p style=color:white> df[(df.country!='us')&(df.UFO_shape=='sphere')|(df.UFO_shape=='light')] </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEGa9RLjhX-2"
      },
      "outputs": [],
      "source": [
        "#6. (Stretch) What is the most common UFO_shape in the United States?\n",
        "\n",
        "#A:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kBVAA_KhX-2"
      },
      "source": [
        "Click here to view solution<br><br>\n",
        "\n",
        "<p style=color:white> df[df.country=='us'].UFO_shape.value_counts() </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsxXtVgBhX-2"
      },
      "source": [
        "# Sorting\n",
        "\n",
        "As we have seen before, sorting our data allows us to view it in new ways. This can be useful in particular for sorting dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYHmKlPehX-2"
      },
      "outputs": [],
      "source": [
        "# The function for sorting dataframes is .sort_values()\n",
        "# You will need to specify which column you are sorting on by adding the 'by' parameter\n",
        "\n",
        "df.sort_values(by='country').head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wannkn5XhX-2"
      },
      "outputs": [],
      "source": [
        "# By default pandas will sort in ascending order, if we want it the other way we need to set ascending to False\n",
        "\n",
        "df.sort_values(by='country',ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heAxdCUMhX-3"
      },
      "outputs": [],
      "source": [
        "# If you want to sort by more than one column, you add them as a list\n",
        "# Note, the order is important here\n",
        "\n",
        "df.sort_values(by=['country','UFO_shape']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCSANegBhX-3"
      },
      "source": [
        "# Changing the Data\n",
        "\n",
        "We have looked at several ways of viewing and exploring our data, but what if want to clean it? In this section we will look at a few ways we can change our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QwBTfUghX-3"
      },
      "outputs": [],
      "source": [
        "# First, let's look at adding a new column. \n",
        "# To do this you use the syntax df['new_column_name']=\n",
        "# You could make the new column a single value for all rows\n",
        "# You could also use a list or series, although it must be the same length as the dataframe itself \n",
        "\n",
        "df['Number']=1\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpVuJF9uhX-3"
      },
      "outputs": [],
      "source": [
        "# To remove a column, you use .drop()\n",
        "# This function works for removing both rows and columns, and is set to rows by default\n",
        "# To remove a column you need to specify you are looking at axis 1 (columns)\n",
        "\n",
        "df.drop('Number',axis=1).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5acpkKhhX-3"
      },
      "source": [
        "Before we move on to other functions we need to discuss a specific property of pandas. When you apply a function to a dataframe you are (with a few exceptions) working on a <b>copy</b>, not the original dataframe itself. This means the outputs you are seeing are only being applied in that one instance. \n",
        "\n",
        "For example, in the last piece of code we removed the new column we created, but if you look at the list of columns below, you will see it is still there..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkAdz39JhX-4"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K0Mt_7QhX-4"
      },
      "source": [
        "The .drop() function only removed the column from the copy, not the original. To apply the function to the original, we need to add the parameter inplace=True.<br><br> Alternatively, you could just redefine the dataframe as this new edit.<br><br> i.e. `df=df.drop('Number',axis=1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrOa6co0hX-4"
      },
      "outputs": [],
      "source": [
        "df.drop('Number',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtofBS1thX-4"
      },
      "source": [
        "Now if we look at our list of column names, we should see 'Number' is gone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7oJgeIHhX-4"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8ytX2SVhX-4"
      },
      "source": [
        "Let's now look at some more useful functions for cleaning/editing our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuVKRsTrhX-5"
      },
      "outputs": [],
      "source": [
        "# If we wanted to drop rows, we still use .drop() but don't add in axis=1\n",
        "# We don't want this change to be permenant, so we'll leave out inplace=True\n",
        "\n",
        "df.drop(0).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg0MpaGahX-5"
      },
      "outputs": [],
      "source": [
        "# There are a couple of methods for renaming columns\n",
        "# This is a useful data cleaning step to ensure all column names are logical, concise and informative\n",
        "# The first method is to simply pass a list the same length as the number of columns, with any edits made there\n",
        "\n",
        "df.columns=['date_time', 'city', 'state_province', 'country', 'UFO_shape',\n",
        "       'length_of_encounter_seconds', 'described_duration_of_encounter',\n",
        "       'description', 'date_documented', 'latitude', 'longitude']\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc-wxTBLhX-5"
      },
      "source": [
        "This is one of the times that doesn't require inplace=True, when you run this code it will affect the original dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9vIzUGhhX-5"
      },
      "outputs": [],
      "source": [
        "# If I only wanted to change specific column names, I can use the .rename() function\n",
        "\n",
        "df.rename(columns={'UFO_shape':'ufo_shape'}).head()\n",
        "\n",
        "# We can use this function to rename multiple columns in the dictionary\n",
        "# However, if we want it to stick we will need to add inplace=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pwRVQPMhX-5"
      },
      "outputs": [],
      "source": [
        "# You may have noticed that when you filter or sort the data the row indices do not reset. \n",
        "# The same thing happens if you delete rows, that specific index will be deleted, but everything else stays the same\n",
        "# This can create issues later if you want to use .iloc, so it is a good idea to reset the index if it has been fractured\n",
        "\n",
        "df.reset_index().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRi6OUf8hX-6"
      },
      "outputs": [],
      "source": [
        "# Notice, this creates a new column called 'index', if you don't want this to be included you instead do:\n",
        "\n",
        "df.reset_index(drop=True).head()\n",
        "\n",
        "# Again, this funciton requires inplace=True for it to permenantly apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxtkPFXzhX-6"
      },
      "outputs": [],
      "source": [
        "# If we wanted to change a specific value within a dataframe (i.e. a cell), we can use .loc or .iloc\n",
        "\n",
        "df.iloc[0,1]='San Marcos'\n",
        "\n",
        "df.iloc[0,1]\n",
        "\n",
        "# Note, .iloc and .loc are powerful functions that do not require inplace=True\n",
        "# Any changes made this way are permenant\n",
        "df.iloc[0,1]='san marcos' #just to change it back..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvvM62I8hX-6"
      },
      "source": [
        "# Data Cleaning\n",
        "\n",
        "We have so far looked at ways we can edit our dataframe in terms of tidying it up, but what about ways to clean up what is within the cells?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Bm2-1jE-hX-6"
      },
      "outputs": [],
      "source": [
        "# Let's first look at changing datatypes, we would expect encounter lengths to be numeric, but the datatype is object\n",
        "print(df.dtypes)\n",
        "\n",
        "# If we wanted to change a data type, we can use .astype() to force the change\n",
        "df.length_of_encounter_seconds.astype('float')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSKdhleChX-6"
      },
      "source": [
        "But there is a problem, some of the values are corrupted. The error shows us which value is the issue. Before we can change the datatype we will need to remove that value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHj9YaBUhX-6"
      },
      "outputs": [],
      "source": [
        "# The function for replacing values is called .replace()\n",
        "# The syntax is to include what you are replacing, and what it now should be\n",
        "# In this instance because we are trying to remove a pesky `, we also need to add in .str to force the series to be a string\n",
        "\n",
        "df.length_of_encounter_seconds.str.replace('`','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkgMEJKhhX-7"
      },
      "outputs": [],
      "source": [
        "# We then redfine the column so it now has this new series\n",
        "\n",
        "df['length_of_encounter_seconds']=df.length_of_encounter_seconds.str.replace('`','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCyGvO3IhX-7"
      },
      "outputs": [],
      "source": [
        "# Now we can change the datatype\n",
        "\n",
        "df['length_of_encounter_seconds']=df.length_of_encounter_seconds.astype('float')\n",
        "\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY842N_khX-7"
      },
      "outputs": [],
      "source": [
        "# Something that can plague our datasets in missing or null values, which pandas calls NaN (Not A Number)\n",
        "# We saw earlier some funtions which shows us how many nulls there are\n",
        "# .isna() will specifically tell you if a value is a null or not\n",
        "\n",
        "df.isna().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj_QX28MhX-7"
      },
      "outputs": [],
      "source": [
        "# This however returns for each cell if it is a null or not\n",
        "# If we want to actually count up how many there are we need to aggregate using .sum()\n",
        "# This function adds up the values in a column (False=0, True=1)\n",
        "\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QzVQvl8hX-7"
      },
      "outputs": [],
      "source": [
        "# But what do we do with nulls? \n",
        "# One thing we can do is remove them using .dropna()\n",
        "\n",
        "print(\"with nulls\")\n",
        "print(df.shape)\n",
        "print(\"without nulls\")\n",
        "print(df.dropna().shape)\n",
        "\n",
        "# You can see that about 14000 rows have been removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TwOEubFhX-7"
      },
      "source": [
        "`.dropna()` is indiscriminate if you leave it as it is, but there are a few parameters that can make it more specific:<br><br>\n",
        "\n",
        "<ul>\n",
        "    <li><code>axis</code> is by default set to 0, so it will drop rows with nulls. If you want it to drop columns, set <code>axis=1</code></li>\n",
        "    <li> It might be that you only want to remove rows/columns that are ALL nulls, so you would set <code>how='all</code></li>\n",
        "    <li> You may only want to remove rows if the null values are in specific columns, you would then use <code>subset=[column_name]</code></li>\n",
        "    <li> Again, this function only works on a copy, for it to be permenant you will need to use <code>inplace=True</code></li>\n",
        "</ul>\n",
        "<br>\n",
        "\n",
        "Note, if a particular column is mostly nulls, it is better to drop the columns than use `.dropna()`.        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNx3HMwNhX-7"
      },
      "outputs": [],
      "source": [
        "# Alternatively, instead of dropping nulls, you could replace them. There are three functions for this:\n",
        "# .fillna() imputes a value you specifiy (useful if the null can be treated as 0)\n",
        "\n",
        "df.fillna(0).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSDFZGuyhX-8"
      },
      "outputs": [],
      "source": [
        "# .ffill() will replace each null with the last valid (non-null) value above it\n",
        "\n",
        "df.ffill().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f9Aa8vOhX-8"
      },
      "outputs": [],
      "source": [
        "# .bfill() will replace each null with the first valid (non-null) value below it\n",
        "\n",
        "df.bfill().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vND2vfHthX-8"
      },
      "source": [
        "Whatever method you choose to remove null values is up to you, make sure the decision is reasonable and within the context of the situation you are working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyuYE-QNhX-8"
      },
      "outputs": [],
      "source": [
        "# Duplicate values can also cause us problems, to check if any rows are duplicates we can use:\n",
        "\n",
        "df.duplicated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsrHuJvYhX-8"
      },
      "outputs": [],
      "source": [
        "# To get a count of how many rows are duplicates, we add .sum()\n",
        "\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHGuHn8bhX-8"
      },
      "outputs": [],
      "source": [
        "# It might be though that you want to check for duplicates within specific columns\n",
        "# We can use the subset parameter again to define which columns to check\n",
        "\n",
        "df.duplicated(subset=['country']).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KN1Ul-G2hX-8"
      },
      "outputs": [],
      "source": [
        "# If we were to find duplicates, and wanted to remove them, we use the .drop_duplicates() function\n",
        "\n",
        "df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCpWREInhX-9"
      },
      "outputs": [],
      "source": [
        "# This function by default only removes duplicates if the entire row is duplicated\n",
        "# It also by default will keep the first instance and drop any subsequent\n",
        "# Here is an example where we drop any row which contains a country already mentioned\n",
        "\n",
        "df.drop_duplicates(subset='country')\n",
        "\n",
        "# (Don't worry, we didn't include inplace=True, the change isn't permenant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUvGPKsAhX-9"
      },
      "source": [
        "# Grouping Data\n",
        "\n",
        "We now know some useful data cleaning tips, but what if we wanted to look at a summary of the data? This is where groupby functions come in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QI5qhE03hX-9"
      },
      "outputs": [],
      "source": [
        "# The syntax for a groupby is to tell it which column to pivot on, and then what aggregate we want\n",
        "# We can use mean, median, max, min, count, sum and std (standard deviation)\n",
        "\n",
        "df.groupby('country').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjZVRgIihX-9"
      },
      "outputs": [],
      "source": [
        "# If we wanted to group by more than one column, we add them as a list\n",
        "\n",
        "df.groupby(['country','UFO_shape']).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SxcueBEhX-9"
      },
      "outputs": [],
      "source": [
        "# By default the groupby will return all columns it can \n",
        "# (it won't return non-numeric columns if the aggregate requires a calculation )\n",
        "# If you want specific columns, call them like you saw earlier\n",
        "\n",
        "df.groupby(['country','UFO_shape']).count()[['city','state_province']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3YdI0NKhX--"
      },
      "source": [
        "# Exporting Data\n",
        "\n",
        "Finally, how do you save your work? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpcALPyDhX--"
      },
      "outputs": [],
      "source": [
        "# Pandas has a function for exporting your data, which is useful if you want to load it into something else like Power BI\n",
        "# It will save the file in the folder you opened the Jupyter notebook, unless you specify a file path\n",
        "# DO NOT NAME YOUR CSV FILE AS THE ONE YOU IMPORTED\n",
        "# This will overwrite your original data, which you should be keeping as a backup\n",
        "\n",
        "df.to_csv('ufo_sightings.csv',index=False)\n",
        "\n",
        "# Top tip, include index=False with this command.\n",
        "# Otherwise, pandas will save the index as a new column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHSdwSywhX--"
      },
      "source": [
        "That ends this notebook, in the next one you will be given an opportunity to clean a dataset using the skills you learned here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4La5Yzb7hX--"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Introducing Dataframes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}